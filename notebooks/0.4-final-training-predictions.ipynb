{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final training and predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all necessary items like before. This time cutting corners with the descriptions - they can be found from earlier notebooks which have identical data loading structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.data.preprocess_data import DatasetManager\n",
    "from src.features.dimension_reduction import fit_pca, load_cnn_embedding\n",
    "\n",
    "# S3 bucket\n",
    "import boto3\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "EMOTION_LIST = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "LOG_PATH = \"/logs\"\n",
    "MODEL_PATH = 'models/resnet50'\n",
    "\n",
    "# Set values\n",
    "BATCH_SIZE = 8\n",
    "VAL_SIZE = 0.2\n",
    "TEST_SIZE = 0.2\n",
    "N_EPOCHS = 100\n",
    "INPUT_SIZE = 224\n",
    "N_FEATURES = len(EMOTION_LIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "torch_manager = DatasetManager(batch_size=BATCH_SIZE, test_size=TEST_SIZE, \n",
    "                        validation_size=VAL_SIZE, transform=test_preprocess, \n",
    "                        test_transform=test_preprocess)\n",
    "\n",
    "train_loader, test_loader, val_loader = torch_manager.load_dataloaders(shuffle_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on test data can be seen as fine-tuning, so let's fine tune with small batch size and small learning rate so observe how our model behaves. https://arxiv.org/pdf/1712.07628.pdf. This paper states that with SGD and correct hyperparameters we should be able to reach better convergence, but they searched for correct hyperparameters for 3 weeks on 16 Tesla K80 GPUs, which demonstrates how hard problem this is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
       "    (4): Dropout(p=0.5, inplace=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=1024, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RESNET_NAME = 'resnet-50.pt'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 2048),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(2048, 1024),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024,  N_FEATURES)\n",
    ")\n",
    "\n",
    "checkpoint = torch.load('../models/' + RESNET_NAME)\n",
    "# checkpoint = load_torch_model(RESNET_NAME)\n",
    "resnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\teemu\\anaconda3\\envs\\pytorchEnv\\lib\\site-packages\\sklearn\\utils\\validation.py:72: FutureWarning: Pass classes=[0. 1. 2. 3. 4. 5. 6.], y=[0. 0. 0. ... 6. 6. 6.] as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error\n",
      "  \"will result in an error\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.SGD(resnet.parameters(), lr=0.00001, momentum=0.9) # LR was dropped from 0.0001 to 0.00001\n",
    "\n",
    "class_weights = torch_manager.calculate_class_weights()\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20: accuracy: 0.625\n",
      "30: accuracy: 0.5846773982048035\n",
      "40: accuracy: 0.5914633870124817\n",
      "50: accuracy: 0.5710784792900085\n",
      "60: accuracy: 0.5778688192367554\n",
      "70: accuracy: 0.5968309640884399\n",
      "80: accuracy: 0.5895061492919922\n",
      "90: accuracy: 0.5769230723381042\n",
      "100: accuracy: 0.5693069100379944\n",
      "110: accuracy: 0.5675675868988037\n",
      "120: accuracy: 0.5702478885650635\n",
      "130: accuracy: 0.569656491279602\n",
      "140: accuracy: 0.5797871947288513\n",
      "150: accuracy: 0.5778145790100098\n",
      "160: accuracy: 0.5799689292907715\n",
      "170: accuracy: 0.5840643048286438\n",
      "180: accuracy: 0.5856353640556335\n",
      "190: accuracy: 0.5837696194648743\n",
      "200: accuracy: 0.5833333134651184\n",
      "210: accuracy: 0.5853080749511719\n",
      "220: accuracy: 0.5825791954994202\n",
      "230: accuracy: 0.5811688303947449\n",
      "240: accuracy: 0.5814315676689148\n",
      "250: accuracy: 0.5801792740821838\n",
      "260: accuracy: 0.5780650973320007\n",
      "270: accuracy: 0.5793358087539673\n",
      "280: accuracy: 0.5765124559402466\n",
      "290: accuracy: 0.5773196220397949\n",
      "300: accuracy: 0.5747508406639099\n",
      "310: accuracy: 0.5755627155303955\n",
      "320: accuracy: 0.5759345889091492\n",
      "330: accuracy: 0.5736404657363892\n",
      "340: accuracy: 0.5747800469398499\n",
      "350: accuracy: 0.5722934603691101\n",
      "360: accuracy: 0.571329653263092\n",
      "370: accuracy: 0.5694069862365723\n",
      "380: accuracy: 0.5682414770126343\n",
      "390: accuracy: 0.5671355724334717\n",
      "400: accuracy: 0.565773069858551\n",
      "410: accuracy: 0.5644769072532654\n",
      "420: accuracy: 0.5629453659057617\n",
      "430: accuracy: 0.5635150671005249\n",
      "440: accuracy: 0.5640589594841003\n",
      "450: accuracy: 0.5631929039955139\n",
      "460: accuracy: 0.5653470754623413\n",
      "470: accuracy: 0.5647558569908142\n",
      "480: accuracy: 0.5639293193817139\n",
      "490: accuracy: 0.5644093751907349\n",
      "500: accuracy: 0.5646207332611084\n",
      "629: accuracy val: 0.8277778029441833\n",
      "629: loss val: 0.7129167318344116\n",
      "TRAINING: epoch: 0, loss: 1.1632177829742432, acc: 0.5646207332611084\n",
      "20: accuracy: 0.5297619104385376\n",
      "30: accuracy: 0.5403225421905518\n",
      "40: accuracy: 0.5670731663703918\n",
      "50: accuracy: 0.5735294222831726\n",
      "60: accuracy: 0.5758196115493774\n",
      "70: accuracy: 0.5880281329154968\n",
      "80: accuracy: 0.5895061492919922\n",
      "90: accuracy: 0.5865384936332703\n",
      "100: accuracy: 0.587871253490448\n",
      "110: accuracy: 0.582207202911377\n",
      "120: accuracy: 0.5805785059928894\n",
      "130: accuracy: 0.5772900581359863\n",
      "140: accuracy: 0.5718085169792175\n",
      "150: accuracy: 0.570364236831665\n",
      "160: accuracy: 0.5745341777801514\n",
      "170: accuracy: 0.5716374516487122\n",
      "180: accuracy: 0.5704420208930969\n",
      "190: accuracy: 0.5667539238929749\n",
      "200: accuracy: 0.5665422677993774\n",
      "210: accuracy: 0.5645734667778015\n",
      "220: accuracy: 0.5639140605926514\n",
      "230: accuracy: 0.5649350881576538\n",
      "240: accuracy: 0.5695021152496338\n",
      "250: accuracy: 0.5662350654602051\n",
      "260: accuracy: 0.5675287246704102\n",
      "270: accuracy: 0.5631918907165527\n",
      "280: accuracy: 0.564056932926178\n",
      "290: accuracy: 0.5657216310501099\n",
      "300: accuracy: 0.5660299062728882\n",
      "310: accuracy: 0.5663183331489563\n",
      "320: accuracy: 0.5673675537109375\n",
      "330: accuracy: 0.5664652585983276\n",
      "340: accuracy: 0.5645161271095276\n",
      "350: accuracy: 0.5648148059844971\n",
      "360: accuracy: 0.5682132840156555\n",
      "370: accuracy: 0.5683962106704712\n",
      "380: accuracy: 0.5698819160461426\n",
      "390: accuracy: 0.5690537095069885\n",
      "400: accuracy: 0.5710723400115967\n",
      "410: accuracy: 0.5723844170570374\n",
      "420: accuracy: 0.5730404257774353\n",
      "430: accuracy: 0.5733758211135864\n",
      "440: accuracy: 0.5722789168357849\n",
      "450: accuracy: 0.5712305903434753\n",
      "460: accuracy: 0.5721257925033569\n",
      "470: accuracy: 0.5719214677810669\n",
      "480: accuracy: 0.5725051760673523\n",
      "490: accuracy: 0.5720468759536743\n",
      "500: accuracy: 0.5738523006439209\n",
      "629: accuracy val: 0.8123015761375427\n",
      "629: loss val: 0.7327876687049866\n",
      "TRAINING: epoch: 1, loss: 1.1245050430297852, acc: 0.5738523006439209\n",
      "20: accuracy: 0.5833333730697632\n",
      "30: accuracy: 0.5645161271095276\n",
      "40: accuracy: 0.5914633870124817\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-082d0a4414ef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "for n_epoch in range(N_EPOCHS):\n",
    "    preds = []\n",
    "    ys = []\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    val_loss = 0\n",
    "    resnet.train()\n",
    "    # here data loaders were changed multiple times between val/test to get even training distributions\n",
    "    for i, batch in enumerate(val_loader):\n",
    "        optim.zero_grad()\n",
    "        X, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        outputs = resnet(X)\n",
    "        y = y.long()\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_history.append(loss.detach())\n",
    "        preds.append(outputs.detach())\n",
    "        ys.append(y.detach())\n",
    "        \n",
    "        if i % 10 == 0 and i > 10:\n",
    "            y_pred = torch.stack([item.topk(1)[-1] for array in preds for item in array]).squeeze()\n",
    "            y_stack = torch.stack([item for array in ys for item in array]).squeeze()\n",
    "            length = len(y_stack)\n",
    "            print(f\"{i}: accuracy: {(y_pred == y_stack).sum() / length}\")\n",
    "\n",
    "        \n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    preds_val = []\n",
    "    ys_val = []\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            X, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            outputs = resnet(X)\n",
    "            y = y.long()\n",
    "            loss = criterion(outputs, y)\n",
    "\n",
    "            val_loss_history.append(loss.detach())\n",
    "            preds_val.append(outputs.detach())\n",
    "            ys_val.append(y.detach())\n",
    "\n",
    "    y_pred_val = torch.stack([item.topk(1)[-1] for array in preds_val for item in array]).squeeze()\n",
    "    y_stack_val = torch.stack([item for array in ys_val for item in array]).squeeze()\n",
    "    length_val = len(y_stack_val)\n",
    "    print(f\"{i}: accuracy val: {(y_pred_val == y_stack_val).sum() / length_val}\")\n",
    "    print(f\"{i}: loss val: {sum(val_loss_history) / len(val_loss_history)}\")\n",
    "\n",
    "    acc = (y_pred == y_stack).sum() / length\n",
    "    \n",
    "    mu_loss_train = sum(loss_history) / len(loss_history)\n",
    "    print(f\"TRAINING: epoch: {n_epoch}, loss: {mu_loss_train}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(resnet, '../models/finetuned-resnet-best-acc.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate once more on specified data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.827579365079365"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = []\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        X, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        outputs = loaded_model(X)\n",
    "        preds = outputs.topk(1)[-1]\n",
    "        y = y.long()\n",
    "        \n",
    "        scores = {'predictions': preds, 'true': y}\n",
    "        predictions.append(scores)\n",
    "        \n",
    "y_true = [element.detach().cpu().numpy() for array in predictions for element in array['true']]\n",
    "y_pred = [element.detach().cpu().numpy() for array in predictions for element in array['predictions']]\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the final predictions for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = torch.load('../models/finetuned-resnet-best-acc.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_directory = '../data/test'\n",
    "imgs = []\n",
    "for img in os.listdir('../data/test'):\n",
    "    loaded_image = Image.open('../data/test/' + img)\n",
    "    imgs.append(np.asarray(loaded_image))\n",
    "stacked_imgs = np.stack(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "resnet.eval()\n",
    "with torch.no_grad():\n",
    "    for X in stacked_imgs:\n",
    "        X = np.stack((X.squeeze(),)*3, axis=-1) # 1 channel to 3 channels\n",
    "        X = test_preprocess(X).unsqueeze(0).to(device)\n",
    "\n",
    "        outputs = loaded_model(X)\n",
    "        preds = outputs.topk(1)[-1]\n",
    "    \n",
    "        test_predictions.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = np.stack([y.detach().cpu().numpy().astype(np.int) for x in test_predictions for y in x]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"../test_results/test_predictions.csv\", final_predictions, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot distribution for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([680.,  98., 485., 865., 648., 472., 790.]),\n",
       " array([0.        , 0.85714286, 1.71428571, 2.57142857, 3.42857143,\n",
       "        4.28571429, 5.14285714, 6.        ]),\n",
       " <a list of 7 Patch objects>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(final_predictions, bins=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks just about right data distribution for our model. Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
