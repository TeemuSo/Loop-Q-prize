{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVtPmUNJf8In"
   },
   "source": [
    "# ResNet-50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E7TSpL_7z_4"
   },
   "source": [
    "Hyperparameter tuning was done manually. Hyperparameters were not searched extensively, due to GPU limits which I run in to quite often. Starting point for hyperparameters were taken from here http://cs230.stanford.edu/projects_winter_2020/reports/32610274.pdf, but different optimizer and parametrization was used in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTE:* Model was trained on Colab, but notebook was refactored to use current project structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FFgtTNQ7g-6"
   },
   "source": [
    "Verify GPU type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkAYC3aRf7GO",
    "outputId": "3bc2be06-2319-417d-e2ce-567344b2e059"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl9G4XiUHeKk"
   },
   "source": [
    "## Preprocess and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x9rMMtuOfIFT",
    "outputId": "383bf1a6-1cf6-45d5-a2f0-4b790873e7dd"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torchvision.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from src.data.preprocess_data import DatasetManager\n",
    "\n",
    "# S3 bucket\n",
    "import boto3\n",
    "from dotenv import dotenv_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QerYjLx_9PGW"
   },
   "source": [
    "Define constants to be used later. *LOG_PATH* and *MODEL_PATH* should be the root path of project and path for models, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "l2qoI6r_gc2u"
   },
   "outputs": [],
   "source": [
    "EMOTION_LIST = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "LOG_PATH = \"/logs\"\n",
    "MODEL_PATH = 'models/resnet50'\n",
    "\n",
    "# Set values\n",
    "BATCH_SIZE = 128\n",
    "VAL_SIZE = 0.2\n",
    "N_EPOCHS = 100\n",
    "INPUT_SIZE = 224\n",
    "N_FEATURES = len(EMOTION_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FEpfZk2v-B_E"
   },
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.RandomAffine(degrees=10, translate=(0.2, 0.2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_preprocess = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(INPUT_SIZE),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset_manager = DatasetManager(batch_size=BATCH_SIZE, test_size=0.2, \n",
    "                        validation_size=0.2, transform=preprocess, \n",
    "                        test_transform=test_preprocess)\n",
    "\n",
    "\n",
    "train_loader, test_loader, val_loader = dataset_manager.load_dataloaders()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iC6YKnSKHa0E"
   },
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgg8_gSuHp6O"
   },
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qxrLkY2PvrwF"
   },
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIfccfP-Htul"
   },
   "source": [
    "Freeze all layers except five latest Bottleneck layer bundles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xydsfD4kHtLx"
   },
   "outputs": [],
   "source": [
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in resnet.layer3[4:].parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qly1iZ9-IGav"
   },
   "source": [
    "Choose device and initialize logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wgXpQpxnv1wC"
   },
   "outputs": [],
   "source": [
    "# Create logging, initialize device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y %H.%m\")\n",
    "writer = SummaryWriter(f'{LOG_PATH}/runs/ResNet-50-{timestamp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So8VbArnIKj3"
   },
   "source": [
    "Replace fully-connected layer to suit our problem. Use heavy Dropout to account for overfitting. Choose *N_FEATURES* as output dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_lBVwrCZwH8P"
   },
   "outputs": [],
   "source": [
    "# Change layers to suit our problem\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Sequential(\n",
    "                nn.Linear(num_ftrs, 2048),\n",
    "                nn.Dropout(0.5),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(1024,  N_FEATURES)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3as5pm4IZqJ"
   },
   "source": [
    "Define optimizer and scheduler for reducing learning rate on plateau. Data is highly imbalanced, so we need to calculate proportional weights for each of the classes, to get equal importance on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9FO6LA-wMnD"
   },
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(resnet.parameters())\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, patience=4)\n",
    "\n",
    "class_weights = dataset_manager.calculate_class_weights()\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B12XCm-uJVmD"
   },
   "source": [
    "Start Tensorboard to monitor convergence. **NOTE**: you need to put your own log directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4JY4-f6B0fz7"
   },
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'ResNet-50-09-06-2021 13.06'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeoPeXv2KeMU"
   },
   "source": [
    "## Train model\n",
    "\n",
    "Early stopping was used with 5 iterations of no improvement, but the early stop was used on training loss. Therefore the best model was handpicked based on Tensorboard metrics, before training loss and validation loss started converging a lot. All models which did improvement were saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4J2_9JquwZ1J",
    "outputId": "a85150f6-8fd7-4c82-9a0d-c8c51300ad4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss: 1.9489587545394897, acc: 0.1640625 \n",
      "iteration: 8, loss: 2.145118236541748, acc: 0.109375 \n",
      "iteration: 16, loss: 2.0745270252227783, acc: 0.1796875 \n",
      "iteration: 24, loss: 2.042987108230591, acc: 0.171875 \n",
      "iteration: 32, loss: 2.0059409141540527, acc: 0.15625 \n",
      "iteration: 40, loss: 1.9749537706375122, acc: 0.1484375 \n",
      "iteration: 48, loss: 1.9587393999099731, acc: 0.203125 \n",
      "iteration: 56, loss: 1.9365102052688599, acc: 0.296875 \n",
      "iteration: 64, loss: 1.9178107976913452, acc: 0.2109375 \n",
      "iteration: 72, loss: 1.9034291505813599, acc: 0.1875 \n",
      "iteration: 80, loss: 1.8984614610671997, acc: 0.21875 \n",
      "iteration: 88, loss: 1.8905425071716309, acc: 0.28125 \n",
      "iteration: 96, loss: 1.882032036781311, acc: 0.2890625 \n",
      "iteration: 104, loss: 1.8758114576339722, acc: 0.1484375 \n",
      "iteration: 112, loss: 1.8670763969421387, acc: 0.3671875 \n",
      "iteration: 120, loss: 1.8614256381988525, acc: 0.21875 \n",
      "TRAINING: epoch: 0, loss: 1.859533667564392, acc: 0.20947265625\n",
      "VALIDATION: epoch: 0, loss: 1.8439150787889957, acc: 0.09765625\n",
      "iteration: 0, loss: 1.736965298652649, acc: 0.140625 \n",
      "iteration: 8, loss: 1.8036187887191772, acc: 0.1875 \n",
      "iteration: 16, loss: 1.7744261026382446, acc: 0.1953125 \n",
      "iteration: 24, loss: 1.7758080959320068, acc: 0.1484375 \n",
      "iteration: 32, loss: 1.7771790027618408, acc: 0.265625 \n",
      "iteration: 40, loss: 1.766160249710083, acc: 0.3046875 \n",
      "iteration: 48, loss: 1.7676422595977783, acc: 0.359375 \n",
      "iteration: 56, loss: 1.7624064683914185, acc: 0.21875 \n",
      "iteration: 64, loss: 1.7614017724990845, acc: 0.28125 \n",
      "iteration: 72, loss: 1.753448486328125, acc: 0.1875 \n",
      "iteration: 80, loss: 1.7489595413208008, acc: 0.3359375 \n",
      "iteration: 88, loss: 1.7417699098587036, acc: 0.3828125 \n",
      "iteration: 96, loss: 1.7351179122924805, acc: 0.3125 \n",
      "iteration: 104, loss: 1.727877140045166, acc: 0.3125 \n",
      "iteration: 112, loss: 1.7278070449829102, acc: 0.3203125 \n",
      "iteration: 120, loss: 1.7227363586425781, acc: 0.328125 \n",
      "TRAINING: epoch: 1, loss: 1.7241754531860352, acc: 0.267578125\n",
      "VALIDATION: epoch: 1, loss: 1.732594609260559, acc: 0.400146484375\n",
      "iteration: 0, loss: 1.6624507904052734, acc: 0.40625 \n",
      "iteration: 8, loss: 1.6853338479995728, acc: 0.3984375 \n",
      "iteration: 16, loss: 1.6810256242752075, acc: 0.2890625 \n",
      "iteration: 24, loss: 1.6882047653198242, acc: 0.3203125 \n",
      "iteration: 32, loss: 1.67668616771698, acc: 0.375 \n",
      "iteration: 40, loss: 1.6743062734603882, acc: 0.421875 \n",
      "iteration: 48, loss: 1.656564474105835, acc: 0.421875 \n",
      "iteration: 56, loss: 1.658052682876587, acc: 0.359375 \n",
      "iteration: 64, loss: 1.6511504650115967, acc: 0.3125 \n",
      "iteration: 72, loss: 1.642745018005371, acc: 0.3046875 \n",
      "iteration: 80, loss: 1.638842225074768, acc: 0.4140625 \n",
      "iteration: 88, loss: 1.6305153369903564, acc: 0.46875 \n",
      "iteration: 96, loss: 1.6217806339263916, acc: 0.4140625 \n",
      "iteration: 104, loss: 1.619307279586792, acc: 0.3984375 \n",
      "iteration: 112, loss: 1.6183750629425049, acc: 0.3828125 \n",
      "iteration: 120, loss: 1.6184345483779907, acc: 0.3515625 \n",
      "TRAINING: epoch: 2, loss: 1.6139984130859375, acc: 0.37744140625\n",
      "VALIDATION: epoch: 2, loss: 1.554914128035307, acc: 0.399169921875\n",
      "iteration: 0, loss: 1.5781952142715454, acc: 0.390625 \n",
      "iteration: 8, loss: 1.5592705011367798, acc: 0.3046875 \n",
      "iteration: 16, loss: 1.5419893264770508, acc: 0.4140625 \n",
      "iteration: 24, loss: 1.5524669885635376, acc: 0.3984375 \n",
      "iteration: 32, loss: 1.5373836755752563, acc: 0.3671875 \n",
      "iteration: 40, loss: 1.5315132141113281, acc: 0.5 \n",
      "iteration: 48, loss: 1.5243600606918335, acc: 0.3828125 \n",
      "iteration: 56, loss: 1.5352511405944824, acc: 0.296875 \n",
      "iteration: 64, loss: 1.5334712266921997, acc: 0.4140625 \n",
      "iteration: 72, loss: 1.5340029001235962, acc: 0.34375 \n",
      "iteration: 80, loss: 1.5352082252502441, acc: 0.53125 \n",
      "iteration: 88, loss: 1.5352556705474854, acc: 0.3984375 \n",
      "iteration: 96, loss: 1.5273468494415283, acc: 0.375 \n",
      "iteration: 104, loss: 1.5208410024642944, acc: 0.390625 \n",
      "iteration: 112, loss: 1.5229120254516602, acc: 0.3984375 \n",
      "iteration: 120, loss: 1.525535225868225, acc: 0.421875 \n",
      "TRAINING: epoch: 3, loss: 1.5285530090332031, acc: 0.3955078125\n",
      "VALIDATION: epoch: 3, loss: 1.5578525625169277, acc: 0.439697265625\n",
      "iteration: 0, loss: 1.5502781867980957, acc: 0.4296875 \n",
      "iteration: 8, loss: 1.4420870542526245, acc: 0.40625 \n",
      "iteration: 16, loss: 1.461815595626831, acc: 0.40625 \n",
      "iteration: 24, loss: 1.4569841623306274, acc: 0.4765625 \n",
      "iteration: 32, loss: 1.4560871124267578, acc: 0.4609375 \n",
      "iteration: 40, loss: 1.4577423334121704, acc: 0.4375 \n",
      "iteration: 48, loss: 1.4511194229125977, acc: 0.4609375 \n",
      "iteration: 56, loss: 1.4479097127914429, acc: 0.4453125 \n",
      "iteration: 64, loss: 1.4411025047302246, acc: 0.4609375 \n",
      "iteration: 72, loss: 1.4425567388534546, acc: 0.4921875 \n",
      "iteration: 80, loss: 1.4482510089874268, acc: 0.3984375 \n",
      "iteration: 88, loss: 1.4512971639633179, acc: 0.515625 \n",
      "iteration: 96, loss: 1.449095368385315, acc: 0.3984375 \n",
      "iteration: 104, loss: 1.4457037448883057, acc: 0.453125 \n",
      "iteration: 112, loss: 1.4458529949188232, acc: 0.421875 \n",
      "iteration: 120, loss: 1.4403799772262573, acc: 0.4296875 \n",
      "TRAINING: epoch: 4, loss: 1.4409674406051636, acc: 0.443359375\n",
      "VALIDATION: epoch: 4, loss: 1.4747334383428097, acc: 0.443115234375\n",
      "iteration: 0, loss: 1.3579857349395752, acc: 0.421875 \n",
      "iteration: 8, loss: 1.369032621383667, acc: 0.4296875 \n",
      "iteration: 16, loss: 1.4178128242492676, acc: 0.453125 \n",
      "iteration: 24, loss: 1.4160480499267578, acc: 0.5703125 \n",
      "iteration: 32, loss: 1.3905179500579834, acc: 0.515625 \n",
      "iteration: 40, loss: 1.3950109481811523, acc: 0.46875 \n",
      "iteration: 48, loss: 1.3983397483825684, acc: 0.3828125 \n",
      "iteration: 56, loss: 1.3870694637298584, acc: 0.5390625 \n",
      "iteration: 64, loss: 1.3840885162353516, acc: 0.4453125 \n",
      "iteration: 72, loss: 1.3862330913543701, acc: 0.375 \n",
      "iteration: 80, loss: 1.3942652940750122, acc: 0.4375 \n",
      "iteration: 88, loss: 1.4003386497497559, acc: 0.453125 \n",
      "iteration: 96, loss: 1.4081403017044067, acc: 0.5078125 \n",
      "iteration: 104, loss: 1.4115002155303955, acc: 0.46875 \n",
      "iteration: 112, loss: 1.413881778717041, acc: 0.4375 \n",
      "iteration: 120, loss: 1.4115073680877686, acc: 0.390625 \n",
      "TRAINING: epoch: 5, loss: 1.4114717245101929, acc: 0.4560546875\n",
      "VALIDATION: epoch: 5, loss: 1.4230574443936348, acc: 0.460205078125\n",
      "iteration: 0, loss: 1.415924310684204, acc: 0.3984375 \n",
      "iteration: 8, loss: 1.400565505027771, acc: 0.4921875 \n",
      "iteration: 16, loss: 1.3502758741378784, acc: 0.46875 \n",
      "iteration: 24, loss: 1.3258349895477295, acc: 0.4140625 \n",
      "iteration: 32, loss: 1.336905837059021, acc: 0.53125 \n",
      "iteration: 40, loss: 1.329498291015625, acc: 0.4921875 \n",
      "iteration: 48, loss: 1.3329353332519531, acc: 0.4765625 \n",
      "iteration: 56, loss: 1.3405506610870361, acc: 0.46875 \n",
      "iteration: 64, loss: 1.3428457975387573, acc: 0.46875 \n",
      "iteration: 72, loss: 1.344775915145874, acc: 0.4296875 \n",
      "iteration: 80, loss: 1.3487571477890015, acc: 0.4765625 \n",
      "iteration: 88, loss: 1.3430805206298828, acc: 0.4921875 \n",
      "iteration: 96, loss: 1.3477325439453125, acc: 0.4609375 \n",
      "iteration: 104, loss: 1.3465399742126465, acc: 0.4296875 \n",
      "iteration: 112, loss: 1.3481419086456299, acc: 0.46875 \n",
      "iteration: 120, loss: 1.350753903388977, acc: 0.4609375 \n",
      "TRAINING: epoch: 6, loss: 1.3497235774993896, acc: 0.46435546875\n",
      "VALIDATION: epoch: 6, loss: 1.4025403782725334, acc: 0.490234375\n",
      "iteration: 0, loss: 1.12014639377594, acc: 0.5234375 \n",
      "iteration: 8, loss: 1.271252155303955, acc: 0.4609375 \n",
      "iteration: 16, loss: 1.284954309463501, acc: 0.421875 \n",
      "iteration: 24, loss: 1.261491060256958, acc: 0.546875 \n",
      "iteration: 32, loss: 1.2705243825912476, acc: 0.5625 \n",
      "iteration: 40, loss: 1.2723780870437622, acc: 0.4296875 \n",
      "iteration: 48, loss: 1.2857939004898071, acc: 0.390625 \n",
      "iteration: 56, loss: 1.2882699966430664, acc: 0.59375 \n",
      "iteration: 64, loss: 1.2881699800491333, acc: 0.5 \n",
      "iteration: 72, loss: 1.2903598546981812, acc: 0.4296875 \n",
      "iteration: 80, loss: 1.2899832725524902, acc: 0.4765625 \n",
      "iteration: 88, loss: 1.2956385612487793, acc: 0.484375 \n",
      "iteration: 96, loss: 1.2974203824996948, acc: 0.4609375 \n",
      "iteration: 104, loss: 1.3026636838912964, acc: 0.4453125 \n",
      "iteration: 112, loss: 1.3046337366104126, acc: 0.4609375 \n",
      "iteration: 120, loss: 1.3052384853363037, acc: 0.4609375 \n",
      "TRAINING: epoch: 7, loss: 1.3095004558563232, acc: 0.47802734375\n",
      "VALIDATION: epoch: 7, loss: 1.4365130178630352, acc: 0.483154296875\n",
      "iteration: 0, loss: 1.2492280006408691, acc: 0.4921875 \n",
      "iteration: 8, loss: 1.3177145719528198, acc: 0.5390625 \n",
      "iteration: 16, loss: 1.2803727388381958, acc: 0.5234375 \n",
      "iteration: 24, loss: 1.2836651802062988, acc: 0.515625 \n",
      "iteration: 32, loss: 1.2812297344207764, acc: 0.46875 \n",
      "iteration: 40, loss: 1.292297124862671, acc: 0.5 \n",
      "iteration: 48, loss: 1.292425274848938, acc: 0.5859375 \n",
      "iteration: 56, loss: 1.2865912914276123, acc: 0.53125 \n",
      "iteration: 64, loss: 1.2793554067611694, acc: 0.4609375 \n",
      "iteration: 72, loss: 1.2883234024047852, acc: 0.5859375 \n",
      "iteration: 80, loss: 1.2915104627609253, acc: 0.5546875 \n",
      "iteration: 88, loss: 1.2951924800872803, acc: 0.5234375 \n",
      "iteration: 96, loss: 1.293761968612671, acc: 0.453125 \n",
      "iteration: 104, loss: 1.2935729026794434, acc: 0.5234375 \n",
      "iteration: 112, loss: 1.3021869659423828, acc: 0.5078125 \n",
      "iteration: 120, loss: 1.2966619729995728, acc: 0.5234375 \n",
      "TRAINING: epoch: 8, loss: 1.295236587524414, acc: 0.51806640625\n",
      "VALIDATION: epoch: 8, loss: 1.4075459837913513, acc: 0.46728515625\n",
      "iteration: 0, loss: 1.1539087295532227, acc: 0.5078125 \n",
      "iteration: 8, loss: 1.260836124420166, acc: 0.546875 \n",
      "iteration: 16, loss: 1.2731901407241821, acc: 0.53125 \n",
      "iteration: 24, loss: 1.2572005987167358, acc: 0.5390625 \n",
      "iteration: 32, loss: 1.2598665952682495, acc: 0.5078125 \n",
      "iteration: 40, loss: 1.25606369972229, acc: 0.4765625 \n",
      "iteration: 48, loss: 1.2532793283462524, acc: 0.5078125 \n",
      "iteration: 56, loss: 1.2590062618255615, acc: 0.4765625 \n",
      "iteration: 64, loss: 1.2587954998016357, acc: 0.453125 \n",
      "iteration: 72, loss: 1.2564854621887207, acc: 0.4921875 \n",
      "iteration: 80, loss: 1.2666431665420532, acc: 0.4453125 \n",
      "iteration: 88, loss: 1.2623445987701416, acc: 0.515625 \n",
      "iteration: 96, loss: 1.2670022249221802, acc: 0.5234375 \n",
      "iteration: 104, loss: 1.267462968826294, acc: 0.4375 \n",
      "iteration: 112, loss: 1.268325924873352, acc: 0.515625 \n",
      "iteration: 120, loss: 1.2689528465270996, acc: 0.5 \n",
      "TRAINING: epoch: 9, loss: 1.2685967683792114, acc: 0.49853515625\n",
      "VALIDATION: epoch: 9, loss: 1.3801515735685825, acc: 0.48681640625\n",
      "iteration: 0, loss: 1.3915094137191772, acc: 0.5 \n",
      "iteration: 8, loss: 1.2142333984375, acc: 0.59375 \n",
      "iteration: 16, loss: 1.1908928155899048, acc: 0.53125 \n",
      "iteration: 24, loss: 1.1966485977172852, acc: 0.484375 \n",
      "iteration: 32, loss: 1.203855037689209, acc: 0.53125 \n",
      "iteration: 40, loss: 1.2141563892364502, acc: 0.4765625 \n",
      "iteration: 48, loss: 1.2254592180252075, acc: 0.5234375 \n",
      "iteration: 56, loss: 1.2262718677520752, acc: 0.5390625 \n",
      "iteration: 64, loss: 1.2234923839569092, acc: 0.5234375 \n",
      "iteration: 72, loss: 1.228502631187439, acc: 0.5 \n",
      "iteration: 80, loss: 1.2374850511550903, acc: 0.4453125 \n",
      "iteration: 88, loss: 1.2404552698135376, acc: 0.484375 \n",
      "iteration: 96, loss: 1.2378182411193848, acc: 0.4765625 \n",
      "iteration: 104, loss: 1.239269733428955, acc: 0.46875 \n",
      "iteration: 112, loss: 1.2398098707199097, acc: 0.4921875 \n",
      "iteration: 120, loss: 1.2339956760406494, acc: 0.5703125 \n",
      "TRAINING: epoch: 10, loss: 1.232529878616333, acc: 0.5087890625\n",
      "VALIDATION: epoch: 10, loss: 1.379500351846218, acc: 0.49169921875\n",
      "iteration: 0, loss: 1.207703948020935, acc: 0.5390625 \n",
      "iteration: 8, loss: 1.2536535263061523, acc: 0.4375 \n",
      "iteration: 16, loss: 1.2348204851150513, acc: 0.515625 \n",
      "iteration: 24, loss: 1.229382872581482, acc: 0.53125 \n",
      "iteration: 32, loss: 1.21180260181427, acc: 0.515625 \n",
      "iteration: 40, loss: 1.211820363998413, acc: 0.59375 \n",
      "iteration: 48, loss: 1.2189795970916748, acc: 0.4921875 \n",
      "iteration: 56, loss: 1.2174103260040283, acc: 0.5078125 \n",
      "iteration: 64, loss: 1.2206767797470093, acc: 0.5546875 \n",
      "iteration: 72, loss: 1.2232718467712402, acc: 0.484375 \n",
      "iteration: 80, loss: 1.218248724937439, acc: 0.5625 \n",
      "iteration: 88, loss: 1.2208776473999023, acc: 0.546875 \n",
      "iteration: 96, loss: 1.218979835510254, acc: 0.5078125 \n",
      "iteration: 104, loss: 1.2195632457733154, acc: 0.5390625 \n",
      "iteration: 112, loss: 1.2102527618408203, acc: 0.5859375 \n",
      "iteration: 120, loss: 1.2068687677383423, acc: 0.578125 \n",
      "TRAINING: epoch: 11, loss: 1.2124035358428955, acc: 0.53076171875\n",
      "VALIDATION: epoch: 11, loss: 1.3836184814572334, acc: 0.49853515625\n",
      "iteration: 0, loss: 1.1717407703399658, acc: 0.4453125 \n",
      "iteration: 8, loss: 1.1545257568359375, acc: 0.5859375 \n",
      "iteration: 16, loss: 1.163936734199524, acc: 0.4765625 \n",
      "iteration: 24, loss: 1.1699810028076172, acc: 0.5546875 \n",
      "iteration: 32, loss: 1.1669195890426636, acc: 0.5078125 \n",
      "iteration: 40, loss: 1.1740365028381348, acc: 0.578125 \n",
      "iteration: 48, loss: 1.1823756694793701, acc: 0.46875 \n",
      "iteration: 56, loss: 1.169729232788086, acc: 0.5859375 \n",
      "iteration: 64, loss: 1.170619010925293, acc: 0.6171875 \n",
      "iteration: 72, loss: 1.1774945259094238, acc: 0.5234375 \n",
      "iteration: 80, loss: 1.1748863458633423, acc: 0.5390625 \n",
      "iteration: 88, loss: 1.1835737228393555, acc: 0.4921875 \n",
      "iteration: 96, loss: 1.181764841079712, acc: 0.5390625 \n",
      "iteration: 104, loss: 1.1801016330718994, acc: 0.546875 \n",
      "iteration: 112, loss: 1.1793979406356812, acc: 0.4609375 \n",
      "iteration: 120, loss: 1.179571509361267, acc: 0.5 \n",
      "TRAINING: epoch: 12, loss: 1.1773804426193237, acc: 0.5263671875\n",
      "VALIDATION: epoch: 12, loss: 1.3787270449101925, acc: 0.504638671875\n",
      "iteration: 0, loss: 1.2231850624084473, acc: 0.4921875 \n",
      "iteration: 8, loss: 1.207480788230896, acc: 0.5390625 \n",
      "iteration: 16, loss: 1.1434751749038696, acc: 0.53125 \n",
      "iteration: 24, loss: 1.1247429847717285, acc: 0.515625 \n",
      "iteration: 32, loss: 1.1318843364715576, acc: 0.5078125 \n",
      "iteration: 40, loss: 1.1424603462219238, acc: 0.5 \n",
      "iteration: 48, loss: 1.1482189893722534, acc: 0.5625 \n",
      "iteration: 56, loss: 1.1491953134536743, acc: 0.640625 \n",
      "iteration: 64, loss: 1.1428145170211792, acc: 0.578125 \n",
      "iteration: 72, loss: 1.160172700881958, acc: 0.484375 \n",
      "iteration: 80, loss: 1.151604175567627, acc: 0.515625 \n",
      "iteration: 88, loss: 1.1476715803146362, acc: 0.5390625 \n",
      "iteration: 96, loss: 1.1433860063552856, acc: 0.515625 \n",
      "iteration: 104, loss: 1.142958402633667, acc: 0.5703125 \n",
      "iteration: 112, loss: 1.1489665508270264, acc: 0.5234375 \n",
      "iteration: 120, loss: 1.1585346460342407, acc: 0.5078125 \n",
      "TRAINING: epoch: 13, loss: 1.1576120853424072, acc: 0.53271484375\n",
      "VALIDATION: epoch: 13, loss: 1.38126615062356, acc: 0.509521484375\n",
      "iteration: 0, loss: 0.9905767440795898, acc: 0.578125 \n",
      "iteration: 8, loss: 1.1581764221191406, acc: 0.546875 \n",
      "iteration: 16, loss: 1.1504842042922974, acc: 0.5234375 \n",
      "iteration: 24, loss: 1.1543561220169067, acc: 0.53125 \n",
      "iteration: 32, loss: 1.1543185710906982, acc: 0.5390625 \n",
      "iteration: 40, loss: 1.1376296281814575, acc: 0.5625 \n",
      "iteration: 48, loss: 1.1302299499511719, acc: 0.625 \n",
      "iteration: 56, loss: 1.127612829208374, acc: 0.5859375 \n",
      "iteration: 64, loss: 1.127205729484558, acc: 0.453125 \n",
      "iteration: 72, loss: 1.130730152130127, acc: 0.5078125 \n",
      "iteration: 80, loss: 1.1224305629730225, acc: 0.6328125 \n",
      "iteration: 88, loss: 1.1241650581359863, acc: 0.5078125 \n",
      "iteration: 96, loss: 1.1270712614059448, acc: 0.4609375 \n",
      "iteration: 104, loss: 1.1281461715698242, acc: 0.546875 \n",
      "iteration: 112, loss: 1.1319705247879028, acc: 0.515625 \n",
      "iteration: 120, loss: 1.1313618421554565, acc: 0.53125 \n",
      "TRAINING: epoch: 14, loss: 1.1320744752883911, acc: 0.54052734375\n",
      "VALIDATION: epoch: 14, loss: 1.3593440055847168, acc: 0.517333984375\n",
      "iteration: 0, loss: 1.202754259109497, acc: 0.5546875 \n",
      "iteration: 8, loss: 1.074034571647644, acc: 0.53125 \n",
      "iteration: 16, loss: 1.0904780626296997, acc: 0.5078125 \n",
      "iteration: 24, loss: 1.1061055660247803, acc: 0.625 \n",
      "iteration: 32, loss: 1.0982377529144287, acc: 0.6015625 \n",
      "iteration: 40, loss: 1.1031041145324707, acc: 0.5 \n",
      "iteration: 48, loss: 1.0958608388900757, acc: 0.6015625 \n",
      "iteration: 56, loss: 1.1057080030441284, acc: 0.625 \n",
      "iteration: 64, loss: 1.115675687789917, acc: 0.640625 \n",
      "iteration: 72, loss: 1.1160281896591187, acc: 0.5703125 \n",
      "iteration: 80, loss: 1.1285755634307861, acc: 0.5234375 \n",
      "iteration: 88, loss: 1.1276979446411133, acc: 0.5703125 \n",
      "iteration: 96, loss: 1.1332924365997314, acc: 0.546875 \n",
      "iteration: 104, loss: 1.1355080604553223, acc: 0.5390625 \n",
      "iteration: 112, loss: 1.134340524673462, acc: 0.5703125 \n",
      "iteration: 120, loss: 1.135310411453247, acc: 0.6328125 \n",
      "TRAINING: epoch: 15, loss: 1.1334946155548096, acc: 0.5712890625\n",
      "VALIDATION: epoch: 15, loss: 1.3445574343204498, acc: 0.4990234375\n",
      "iteration: 0, loss: 0.9971677660942078, acc: 0.640625 \n",
      "iteration: 8, loss: 1.1189219951629639, acc: 0.5 \n",
      "iteration: 16, loss: 1.1179900169372559, acc: 0.5234375 \n",
      "iteration: 24, loss: 1.1283544301986694, acc: 0.484375 \n",
      "iteration: 32, loss: 1.1277509927749634, acc: 0.546875 \n",
      "iteration: 40, loss: 1.1180647611618042, acc: 0.5078125 \n",
      "iteration: 48, loss: 1.112900972366333, acc: 0.5859375 \n",
      "iteration: 56, loss: 1.1073931455612183, acc: 0.5703125 \n",
      "iteration: 64, loss: 1.1060986518859863, acc: 0.515625 \n",
      "iteration: 72, loss: 1.1038203239440918, acc: 0.5390625 \n",
      "iteration: 80, loss: 1.1082780361175537, acc: 0.5859375 \n",
      "iteration: 88, loss: 1.1094752550125122, acc: 0.6171875 \n",
      "iteration: 96, loss: 1.1125502586364746, acc: 0.5078125 \n",
      "iteration: 104, loss: 1.1133946180343628, acc: 0.546875 \n",
      "iteration: 112, loss: 1.112823724746704, acc: 0.5859375 \n",
      "iteration: 120, loss: 1.1131749153137207, acc: 0.46875 \n",
      "TRAINING: epoch: 16, loss: 1.1192115545272827, acc: 0.54541015625\n",
      "VALIDATION: epoch: 16, loss: 1.4625632017850876, acc: 0.485595703125\n",
      "iteration: 0, loss: 1.0459662675857544, acc: 0.5859375 \n",
      "iteration: 8, loss: 1.0772243738174438, acc: 0.625 \n",
      "iteration: 16, loss: 1.0932364463806152, acc: 0.546875 \n",
      "iteration: 24, loss: 1.0702292919158936, acc: 0.609375 \n",
      "iteration: 32, loss: 1.0638409852981567, acc: 0.5234375 \n",
      "iteration: 40, loss: 1.0695356130599976, acc: 0.546875 \n",
      "iteration: 48, loss: 1.0869758129119873, acc: 0.515625 \n",
      "iteration: 56, loss: 1.0803852081298828, acc: 0.4765625 \n",
      "iteration: 64, loss: 1.0843364000320435, acc: 0.53125 \n",
      "iteration: 72, loss: 1.0897542238235474, acc: 0.5703125 \n",
      "iteration: 80, loss: 1.0861506462097168, acc: 0.5703125 \n",
      "iteration: 88, loss: 1.0849934816360474, acc: 0.5703125 \n",
      "iteration: 96, loss: 1.0956814289093018, acc: 0.5390625 \n",
      "iteration: 104, loss: 1.0926390886306763, acc: 0.5625 \n",
      "iteration: 112, loss: 1.0976213216781616, acc: 0.5546875 \n",
      "iteration: 120, loss: 1.0940289497375488, acc: 0.671875 \n",
      "TRAINING: epoch: 17, loss: 1.0977483987808228, acc: 0.5625\n",
      "VALIDATION: epoch: 17, loss: 1.4069856852293015, acc: 0.53369140625\n",
      "iteration: 0, loss: 1.2430342435836792, acc: 0.578125 \n",
      "iteration: 8, loss: 1.0780935287475586, acc: 0.609375 \n",
      "iteration: 16, loss: 1.0806431770324707, acc: 0.6484375 \n",
      "iteration: 24, loss: 1.097187876701355, acc: 0.515625 \n",
      "iteration: 32, loss: 1.0919855833053589, acc: 0.6171875 \n",
      "iteration: 40, loss: 1.094728946685791, acc: 0.5390625 \n",
      "iteration: 48, loss: 1.0843673944473267, acc: 0.5078125 \n",
      "iteration: 56, loss: 1.0901473760604858, acc: 0.5625 \n",
      "iteration: 64, loss: 1.0834531784057617, acc: 0.5859375 \n",
      "iteration: 72, loss: 1.0844212770462036, acc: 0.5625 \n",
      "iteration: 80, loss: 1.087411880493164, acc: 0.546875 \n",
      "iteration: 88, loss: 1.0841907262802124, acc: 0.578125 \n",
      "iteration: 96, loss: 1.0819909572601318, acc: 0.578125 \n",
      "iteration: 104, loss: 1.0767935514450073, acc: 0.5546875 \n",
      "iteration: 112, loss: 1.0717893838882446, acc: 0.5703125 \n",
      "iteration: 120, loss: 1.0725668668746948, acc: 0.5859375 \n",
      "TRAINING: epoch: 18, loss: 1.0716314315795898, acc: 0.5712890625\n",
      "VALIDATION: epoch: 18, loss: 1.372906856238842, acc: 0.523193359375\n",
      "iteration: 0, loss: 0.9697602391242981, acc: 0.5390625 \n",
      "iteration: 8, loss: 0.9796159863471985, acc: 0.5703125 \n",
      "iteration: 16, loss: 1.0043447017669678, acc: 0.5546875 \n",
      "iteration: 24, loss: 1.0075228214263916, acc: 0.640625 \n",
      "iteration: 32, loss: 1.0088238716125488, acc: 0.5234375 \n",
      "iteration: 40, loss: 1.0251461267471313, acc: 0.546875 \n",
      "iteration: 48, loss: 1.024379849433899, acc: 0.5625 \n",
      "iteration: 56, loss: 1.027535319328308, acc: 0.6171875 \n",
      "iteration: 64, loss: 1.0255839824676514, acc: 0.609375 \n",
      "iteration: 72, loss: 1.0277436971664429, acc: 0.65625 \n",
      "iteration: 80, loss: 1.0293774604797363, acc: 0.5546875 \n",
      "iteration: 88, loss: 1.0405429601669312, acc: 0.546875 \n",
      "iteration: 96, loss: 1.036135196685791, acc: 0.6328125 \n",
      "iteration: 104, loss: 1.0476243495941162, acc: 0.5859375 \n",
      "iteration: 112, loss: 1.0431112051010132, acc: 0.6015625 \n",
      "iteration: 120, loss: 1.0416985750198364, acc: 0.59375 \n",
      "TRAINING: epoch: 19, loss: 1.047451138496399, acc: 0.58349609375\n",
      "VALIDATION: epoch: 19, loss: 1.398159071803093, acc: 0.48486328125\n",
      "iteration: 0, loss: 0.8274966478347778, acc: 0.6328125 \n",
      "iteration: 8, loss: 0.9525421857833862, acc: 0.5703125 \n",
      "iteration: 16, loss: 1.0003186464309692, acc: 0.5390625 \n",
      "iteration: 24, loss: 1.0312080383300781, acc: 0.546875 \n",
      "iteration: 32, loss: 1.0364993810653687, acc: 0.5625 \n",
      "iteration: 40, loss: 1.0395830869674683, acc: 0.5703125 \n",
      "iteration: 48, loss: 1.0411148071289062, acc: 0.59375 \n",
      "iteration: 56, loss: 1.0496797561645508, acc: 0.6640625 \n",
      "iteration: 64, loss: 1.0499162673950195, acc: 0.5703125 \n",
      "iteration: 72, loss: 1.0519061088562012, acc: 0.546875 \n",
      "iteration: 80, loss: 1.0522689819335938, acc: 0.53125 \n",
      "iteration: 88, loss: 1.055606484413147, acc: 0.6328125 \n",
      "iteration: 96, loss: 1.0534539222717285, acc: 0.5234375 \n",
      "iteration: 104, loss: 1.0529780387878418, acc: 0.5234375 \n",
      "iteration: 112, loss: 1.05579674243927, acc: 0.5234375 \n",
      "iteration: 120, loss: 1.0484719276428223, acc: 0.65625 \n",
      "TRAINING: epoch: 20, loss: 1.0476669073104858, acc: 0.57421875\n",
      "VALIDATION: epoch: 20, loss: 1.3599349409341812, acc: 0.531494140625\n",
      "iteration: 0, loss: 1.0428121089935303, acc: 0.6015625 \n",
      "iteration: 8, loss: 0.981939435005188, acc: 0.5859375 \n",
      "iteration: 16, loss: 0.9960978031158447, acc: 0.5390625 \n",
      "iteration: 24, loss: 1.0049043893814087, acc: 0.625 \n",
      "iteration: 32, loss: 1.0067639350891113, acc: 0.5546875 \n",
      "iteration: 40, loss: 1.0109227895736694, acc: 0.4921875 \n",
      "iteration: 48, loss: 1.0136194229125977, acc: 0.546875 \n",
      "iteration: 56, loss: 1.0128809213638306, acc: 0.6171875 \n",
      "iteration: 64, loss: 1.016165018081665, acc: 0.5 \n",
      "iteration: 72, loss: 1.014155387878418, acc: 0.578125 \n",
      "iteration: 80, loss: 1.0093454122543335, acc: 0.625 \n",
      "iteration: 88, loss: 1.0096231698989868, acc: 0.609375 \n",
      "iteration: 96, loss: 1.0081504583358765, acc: 0.6015625 \n",
      "iteration: 104, loss: 0.9988208413124084, acc: 0.6328125 \n",
      "iteration: 112, loss: 0.9939702153205872, acc: 0.578125 \n",
      "iteration: 120, loss: 0.9916713237762451, acc: 0.53125 \n",
      "TRAINING: epoch: 21, loss: 0.9930047392845154, acc: 0.576171875\n",
      "VALIDATION: epoch: 21, loss: 1.3112580087035894, acc: 0.56298828125\n",
      "iteration: 0, loss: 1.1009429693222046, acc: 0.59375 \n",
      "iteration: 8, loss: 0.984766960144043, acc: 0.703125 \n",
      "iteration: 16, loss: 0.9930974245071411, acc: 0.65625 \n",
      "iteration: 24, loss: 0.9726347327232361, acc: 0.59375 \n",
      "iteration: 32, loss: 0.9808489680290222, acc: 0.5859375 \n",
      "iteration: 40, loss: 0.9851170778274536, acc: 0.6015625 \n",
      "iteration: 48, loss: 0.9860477447509766, acc: 0.5703125 \n",
      "iteration: 56, loss: 0.9981414079666138, acc: 0.5546875 \n",
      "iteration: 64, loss: 0.993930995464325, acc: 0.6484375 \n",
      "iteration: 72, loss: 0.9890798330307007, acc: 0.65625 \n",
      "iteration: 80, loss: 0.9900677800178528, acc: 0.59375 \n",
      "iteration: 88, loss: 0.9867010116577148, acc: 0.5703125 \n",
      "iteration: 96, loss: 0.9872167110443115, acc: 0.578125 \n",
      "iteration: 104, loss: 0.9781405925750732, acc: 0.609375 \n",
      "iteration: 112, loss: 0.9727655649185181, acc: 0.5859375 \n",
      "iteration: 120, loss: 0.9666873812675476, acc: 0.5390625 \n",
      "TRAINING: epoch: 22, loss: 0.9651440978050232, acc: 0.6025390625\n",
      "VALIDATION: epoch: 22, loss: 1.3297078683972359, acc: 0.56396484375\n",
      "iteration: 0, loss: 0.7845082879066467, acc: 0.6328125 \n",
      "iteration: 8, loss: 0.842806339263916, acc: 0.6328125 \n",
      "iteration: 16, loss: 0.8796483874320984, acc: 0.6796875 \n",
      "iteration: 24, loss: 0.899707019329071, acc: 0.6015625 \n",
      "iteration: 32, loss: 0.9308830499649048, acc: 0.6328125 \n",
      "iteration: 40, loss: 0.9233380556106567, acc: 0.625 \n",
      "iteration: 48, loss: 0.9301669597625732, acc: 0.6484375 \n",
      "iteration: 56, loss: 0.9336864352226257, acc: 0.6015625 \n",
      "iteration: 64, loss: 0.9310581088066101, acc: 0.59375 \n",
      "iteration: 72, loss: 0.9296338558197021, acc: 0.6796875 \n",
      "iteration: 80, loss: 0.931422233581543, acc: 0.5859375 \n",
      "iteration: 88, loss: 0.9334198832511902, acc: 0.5703125 \n",
      "iteration: 96, loss: 0.9339969158172607, acc: 0.5546875 \n",
      "iteration: 104, loss: 0.936834454536438, acc: 0.546875 \n",
      "iteration: 112, loss: 0.9376835823059082, acc: 0.703125 \n",
      "iteration: 120, loss: 0.93644779920578, acc: 0.640625 \n",
      "TRAINING: epoch: 23, loss: 0.9363104104995728, acc: 0.62060546875\n",
      "VALIDATION: epoch: 23, loss: 1.3393563907593489, acc: 0.570556640625\n",
      "iteration: 0, loss: 1.0334151983261108, acc: 0.578125 \n",
      "iteration: 8, loss: 0.9489641189575195, acc: 0.578125 \n",
      "iteration: 16, loss: 0.9580473303794861, acc: 0.6796875 \n",
      "iteration: 24, loss: 0.9474415183067322, acc: 0.6640625 \n",
      "iteration: 32, loss: 0.9453728795051575, acc: 0.6015625 \n",
      "iteration: 40, loss: 0.9437608122825623, acc: 0.59375 \n",
      "iteration: 48, loss: 0.9421167969703674, acc: 0.640625 \n",
      "iteration: 56, loss: 0.9427180886268616, acc: 0.59375 \n",
      "iteration: 64, loss: 0.9384167790412903, acc: 0.515625 \n",
      "iteration: 72, loss: 0.9357775449752808, acc: 0.59375 \n",
      "iteration: 80, loss: 0.9394468069076538, acc: 0.609375 \n",
      "iteration: 88, loss: 0.9320884346961975, acc: 0.578125 \n",
      "iteration: 96, loss: 0.9301308989524841, acc: 0.65625 \n",
      "iteration: 104, loss: 0.9247710108757019, acc: 0.6640625 \n",
      "iteration: 112, loss: 0.9233434796333313, acc: 0.578125 \n",
      "iteration: 120, loss: 0.9254971146583557, acc: 0.5703125 \n",
      "TRAINING: epoch: 24, loss: 0.9264766573905945, acc: 0.60595703125\n",
      "VALIDATION: epoch: 24, loss: 1.2978532370179892, acc: 0.57421875\n",
      "iteration: 0, loss: 0.9568866491317749, acc: 0.5859375 \n",
      "iteration: 8, loss: 0.9428751468658447, acc: 0.6875 \n",
      "iteration: 16, loss: 0.912371814250946, acc: 0.59375 \n",
      "iteration: 24, loss: 0.9069353938102722, acc: 0.6171875 \n",
      "iteration: 32, loss: 0.9151445627212524, acc: 0.6640625 \n",
      "iteration: 40, loss: 0.9139874577522278, acc: 0.6171875 \n",
      "iteration: 48, loss: 0.9050283432006836, acc: 0.6171875 \n",
      "iteration: 56, loss: 0.8986150026321411, acc: 0.6328125 \n",
      "iteration: 64, loss: 0.8993870615959167, acc: 0.625 \n",
      "iteration: 72, loss: 0.8989628553390503, acc: 0.609375 \n",
      "iteration: 80, loss: 0.9042608141899109, acc: 0.6640625 \n",
      "iteration: 88, loss: 0.9136248230934143, acc: 0.6171875 \n",
      "iteration: 96, loss: 0.9170204997062683, acc: 0.671875 \n",
      "iteration: 104, loss: 0.9177700281143188, acc: 0.59375 \n",
      "iteration: 112, loss: 0.9215590357780457, acc: 0.6171875 \n",
      "iteration: 120, loss: 0.9192185997962952, acc: 0.6171875 \n",
      "TRAINING: epoch: 25, loss: 0.9181270599365234, acc: 0.626953125\n",
      "VALIDATION: epoch: 25, loss: 1.311163853853941, acc: 0.57177734375\n",
      "iteration: 0, loss: 0.9821450710296631, acc: 0.5546875 \n",
      "iteration: 8, loss: 0.9034810662269592, acc: 0.640625 \n",
      "iteration: 16, loss: 0.9210288524627686, acc: 0.609375 \n",
      "iteration: 24, loss: 0.9324679970741272, acc: 0.640625 \n",
      "iteration: 32, loss: 0.9283809065818787, acc: 0.5625 \n",
      "iteration: 40, loss: 0.9359720349311829, acc: 0.6484375 \n",
      "iteration: 48, loss: 0.9244964718818665, acc: 0.671875 \n",
      "iteration: 56, loss: 0.9214906692504883, acc: 0.5625 \n",
      "iteration: 64, loss: 0.9295738935470581, acc: 0.6953125 \n",
      "iteration: 72, loss: 0.919701337814331, acc: 0.7265625 \n",
      "iteration: 80, loss: 0.9141706228256226, acc: 0.6171875 \n",
      "iteration: 88, loss: 0.9135642647743225, acc: 0.625 \n",
      "iteration: 96, loss: 0.9104923605918884, acc: 0.5703125 \n",
      "iteration: 104, loss: 0.9113714694976807, acc: 0.671875 \n",
      "iteration: 112, loss: 0.9059025645256042, acc: 0.6328125 \n",
      "iteration: 120, loss: 0.9082298278808594, acc: 0.59375 \n",
      "TRAINING: epoch: 26, loss: 0.9086106419563293, acc: 0.62646484375\n",
      "VALIDATION: epoch: 26, loss: 1.2990766409784555, acc: 0.5810546875\n",
      "iteration: 0, loss: 1.0056631565093994, acc: 0.609375 \n",
      "iteration: 8, loss: 0.8663633465766907, acc: 0.7109375 \n",
      "iteration: 16, loss: 0.8713517785072327, acc: 0.671875 \n",
      "iteration: 24, loss: 0.8767433762550354, acc: 0.5546875 \n",
      "iteration: 32, loss: 0.8819378018379211, acc: 0.7265625 \n",
      "iteration: 40, loss: 0.8901228904724121, acc: 0.65625 \n",
      "iteration: 48, loss: 0.8971342444419861, acc: 0.5859375 \n",
      "iteration: 56, loss: 0.9026321172714233, acc: 0.59375 \n",
      "iteration: 64, loss: 0.9084693789482117, acc: 0.6328125 \n",
      "iteration: 72, loss: 0.8985562324523926, acc: 0.5859375 \n",
      "iteration: 80, loss: 0.9073843359947205, acc: 0.5390625 \n",
      "iteration: 88, loss: 0.9018592238426208, acc: 0.5703125 \n",
      "iteration: 96, loss: 0.9061437249183655, acc: 0.59375 \n",
      "iteration: 104, loss: 0.9033645987510681, acc: 0.5625 \n",
      "iteration: 112, loss: 0.9058122634887695, acc: 0.6875 \n",
      "iteration: 120, loss: 0.9019649028778076, acc: 0.5859375 \n",
      "TRAINING: epoch: 27, loss: 0.9042879343032837, acc: 0.61669921875\n",
      "VALIDATION: epoch: 27, loss: 1.3829120192676783, acc: 0.577392578125\n",
      "iteration: 0, loss: 1.2085641622543335, acc: 0.5859375 \n",
      "iteration: 8, loss: 0.8894824981689453, acc: 0.6484375 \n",
      "iteration: 16, loss: 0.9170582890510559, acc: 0.5390625 \n",
      "iteration: 24, loss: 0.9182541370391846, acc: 0.6328125 \n",
      "iteration: 32, loss: 0.9064184427261353, acc: 0.6484375 \n",
      "iteration: 40, loss: 0.8877364993095398, acc: 0.6171875 \n",
      "iteration: 48, loss: 0.8883838057518005, acc: 0.578125 \n",
      "iteration: 56, loss: 0.8945971727371216, acc: 0.625 \n",
      "iteration: 64, loss: 0.9003021717071533, acc: 0.625 \n",
      "iteration: 72, loss: 0.8997057676315308, acc: 0.6328125 \n",
      "iteration: 80, loss: 0.9052748680114746, acc: 0.5859375 \n",
      "iteration: 88, loss: 0.9027422070503235, acc: 0.6875 \n",
      "iteration: 96, loss: 0.9040316343307495, acc: 0.5625 \n",
      "iteration: 104, loss: 0.9083737134933472, acc: 0.6171875 \n",
      "iteration: 112, loss: 0.9099856615066528, acc: 0.5703125 \n",
      "iteration: 120, loss: 0.9097468852996826, acc: 0.6484375 \n",
      "TRAINING: epoch: 28, loss: 0.9085275530815125, acc: 0.61279296875\n",
      "VALIDATION: epoch: 28, loss: 1.3172716982662678, acc: 0.5654296875\n",
      "iteration: 0, loss: 0.6476247310638428, acc: 0.6875 \n",
      "iteration: 8, loss: 0.8491796851158142, acc: 0.6640625 \n",
      "iteration: 16, loss: 0.864804208278656, acc: 0.609375 \n",
      "iteration: 24, loss: 0.8889532089233398, acc: 0.671875 \n",
      "iteration: 32, loss: 0.8900641798973083, acc: 0.6484375 \n",
      "iteration: 40, loss: 0.8784185647964478, acc: 0.671875 \n",
      "iteration: 48, loss: 0.8792091012001038, acc: 0.5703125 \n",
      "iteration: 56, loss: 0.8803108930587769, acc: 0.6484375 \n",
      "iteration: 64, loss: 0.8791147470474243, acc: 0.6484375 \n",
      "iteration: 72, loss: 0.8813884854316711, acc: 0.6328125 \n",
      "iteration: 80, loss: 0.8795321583747864, acc: 0.703125 \n",
      "iteration: 88, loss: 0.8883860111236572, acc: 0.5390625 \n",
      "iteration: 96, loss: 0.8899024724960327, acc: 0.65625 \n",
      "iteration: 104, loss: 0.892589807510376, acc: 0.625 \n",
      "iteration: 112, loss: 0.8957350254058838, acc: 0.5859375 \n",
      "iteration: 120, loss: 0.8975025415420532, acc: 0.640625 \n",
      "TRAINING: epoch: 29, loss: 0.8960205912590027, acc: 0.6376953125\n",
      "VALIDATION: epoch: 29, loss: 1.3378084562718868, acc: 0.576904296875\n",
      "iteration: 0, loss: 0.7408969402313232, acc: 0.6875 \n",
      "iteration: 8, loss: 0.8584614396095276, acc: 0.609375 \n",
      "iteration: 16, loss: 0.8981818556785583, acc: 0.5390625 \n",
      "iteration: 24, loss: 0.8888958692550659, acc: 0.6875 \n",
      "iteration: 32, loss: 0.8776459693908691, acc: 0.671875 \n",
      "iteration: 40, loss: 0.8980386257171631, acc: 0.5546875 \n",
      "iteration: 48, loss: 0.8892457485198975, acc: 0.703125 \n",
      "iteration: 56, loss: 0.8844932317733765, acc: 0.640625 \n",
      "iteration: 64, loss: 0.8847845196723938, acc: 0.671875 \n",
      "iteration: 72, loss: 0.8823336362838745, acc: 0.625 \n",
      "iteration: 80, loss: 0.8846742510795593, acc: 0.609375 \n",
      "iteration: 88, loss: 0.8880706429481506, acc: 0.5625 \n",
      "iteration: 96, loss: 0.8908588886260986, acc: 0.578125 \n",
      "iteration: 104, loss: 0.8853336572647095, acc: 0.6328125 \n",
      "iteration: 112, loss: 0.8863983154296875, acc: 0.640625 \n",
      "iteration: 120, loss: 0.884289562702179, acc: 0.671875 \n",
      "TRAINING: epoch: 30, loss: 0.8856932520866394, acc: 0.63037109375\n",
      "VALIDATION: epoch: 30, loss: 1.343722976744175, acc: 0.5849609375\n",
      "iteration: 0, loss: 0.936945915222168, acc: 0.578125 \n",
      "iteration: 8, loss: 0.9049143195152283, acc: 0.640625 \n",
      "iteration: 16, loss: 0.8808664083480835, acc: 0.6328125 \n",
      "iteration: 24, loss: 0.9028641581535339, acc: 0.6171875 \n",
      "iteration: 32, loss: 0.8961572051048279, acc: 0.640625 \n",
      "iteration: 40, loss: 0.8905528783798218, acc: 0.6015625 \n",
      "iteration: 48, loss: 0.8869156837463379, acc: 0.71875 \n",
      "iteration: 56, loss: 0.8890892863273621, acc: 0.65625 \n",
      "iteration: 64, loss: 0.8909623026847839, acc: 0.6171875 \n",
      "iteration: 72, loss: 0.887834906578064, acc: 0.625 \n",
      "iteration: 80, loss: 0.8812664747238159, acc: 0.640625 \n",
      "iteration: 88, loss: 0.8842676877975464, acc: 0.6171875 \n",
      "iteration: 96, loss: 0.8880231976509094, acc: 0.6171875 \n",
      "iteration: 104, loss: 0.8905113339424133, acc: 0.6328125 \n",
      "iteration: 112, loss: 0.8871567249298096, acc: 0.71875 \n",
      "iteration: 120, loss: 0.8837146759033203, acc: 0.671875 \n",
      "TRAINING: epoch: 31, loss: 0.8822983503341675, acc: 0.63916015625\n",
      "VALIDATION: epoch: 31, loss: 1.3414589557796717, acc: 0.579345703125\n",
      "iteration: 0, loss: 0.736495316028595, acc: 0.6953125 \n",
      "iteration: 8, loss: 0.9043484926223755, acc: 0.6328125 \n",
      "iteration: 16, loss: 0.8829591870307922, acc: 0.625 \n",
      "iteration: 24, loss: 0.8868882656097412, acc: 0.71875 \n",
      "iteration: 32, loss: 0.8989583253860474, acc: 0.5859375 \n",
      "iteration: 40, loss: 0.9008623361587524, acc: 0.6796875 \n",
      "iteration: 48, loss: 0.8900536298751831, acc: 0.640625 \n",
      "iteration: 56, loss: 0.8804991245269775, acc: 0.6328125 \n",
      "iteration: 64, loss: 0.8876389861106873, acc: 0.671875 \n",
      "iteration: 72, loss: 0.8873652815818787, acc: 0.578125 \n",
      "iteration: 80, loss: 0.884638786315918, acc: 0.6328125 \n",
      "iteration: 88, loss: 0.8840773701667786, acc: 0.6015625 \n",
      "iteration: 96, loss: 0.8828878402709961, acc: 0.5546875 \n",
      "iteration: 104, loss: 0.8811959028244019, acc: 0.6640625 \n",
      "iteration: 112, loss: 0.8812392354011536, acc: 0.625 \n",
      "iteration: 120, loss: 0.8802186846733093, acc: 0.6328125 \n",
      "TRAINING: epoch: 32, loss: 0.8804981708526611, acc: 0.6357421875\n",
      "VALIDATION: epoch: 32, loss: 1.3519619759172201, acc: 0.574462890625\n",
      "iteration: 0, loss: 0.9107966423034668, acc: 0.609375 \n",
      "iteration: 8, loss: 0.8577511310577393, acc: 0.6484375 \n",
      "iteration: 16, loss: 0.8797739744186401, acc: 0.6015625 \n",
      "iteration: 24, loss: 0.8772050142288208, acc: 0.5859375 \n",
      "iteration: 32, loss: 0.861621081829071, acc: 0.6953125 \n",
      "iteration: 40, loss: 0.8779608011245728, acc: 0.65625 \n",
      "iteration: 48, loss: 0.8702561259269714, acc: 0.6640625 \n",
      "iteration: 56, loss: 0.8768750429153442, acc: 0.6484375 \n",
      "iteration: 64, loss: 0.8769187927246094, acc: 0.578125 \n",
      "iteration: 72, loss: 0.8762157559394836, acc: 0.65625 \n",
      "iteration: 80, loss: 0.8785865902900696, acc: 0.6328125 \n",
      "iteration: 88, loss: 0.8810579180717468, acc: 0.65625 \n",
      "iteration: 96, loss: 0.8896524906158447, acc: 0.625 \n",
      "iteration: 104, loss: 0.8879950046539307, acc: 0.671875 \n",
      "iteration: 112, loss: 0.890487790107727, acc: 0.5703125 \n",
      "iteration: 120, loss: 0.8921909928321838, acc: 0.640625 \n",
      "TRAINING: epoch: 33, loss: 0.8901908993721008, acc: 0.6337890625\n",
      "VALIDATION: epoch: 33, loss: 1.357767729088664, acc: 0.58154296875\n",
      "iteration: 0, loss: 0.9666035175323486, acc: 0.578125 \n",
      "iteration: 8, loss: 0.9116166234016418, acc: 0.6875 \n",
      "iteration: 16, loss: 0.8778318762779236, acc: 0.65625 \n",
      "iteration: 24, loss: 0.8905841708183289, acc: 0.609375 \n",
      "iteration: 32, loss: 0.8717460036277771, acc: 0.6640625 \n",
      "iteration: 40, loss: 0.8696103692054749, acc: 0.7265625 \n",
      "iteration: 48, loss: 0.876736581325531, acc: 0.59375 \n",
      "iteration: 56, loss: 0.8667303323745728, acc: 0.5859375 \n",
      "iteration: 64, loss: 0.8680910468101501, acc: 0.515625 \n",
      "iteration: 72, loss: 0.8748561143875122, acc: 0.59375 \n",
      "iteration: 80, loss: 0.8730791807174683, acc: 0.6796875 \n",
      "iteration: 88, loss: 0.8693251013755798, acc: 0.625 \n",
      "iteration: 96, loss: 0.8676164746284485, acc: 0.625 \n",
      "iteration: 104, loss: 0.8696410059928894, acc: 0.640625 \n",
      "iteration: 112, loss: 0.8712172508239746, acc: 0.5859375 \n",
      "iteration: 120, loss: 0.8740937113761902, acc: 0.578125 \n",
      "TRAINING: epoch: 34, loss: 0.8757009506225586, acc: 0.62158203125\n",
      "VALIDATION: epoch: 34, loss: 1.3672939408570528, acc: 0.571533203125\n",
      "iteration: 0, loss: 1.039508581161499, acc: 0.5859375 \n",
      "iteration: 8, loss: 0.9106901288032532, acc: 0.609375 \n",
      "iteration: 16, loss: 0.8972110152244568, acc: 0.65625 \n",
      "iteration: 24, loss: 0.8917267918586731, acc: 0.6015625 \n",
      "iteration: 32, loss: 0.8749356269836426, acc: 0.703125 \n",
      "iteration: 40, loss: 0.868385374546051, acc: 0.578125 \n",
      "iteration: 48, loss: 0.8715286254882812, acc: 0.59375 \n",
      "iteration: 56, loss: 0.8709825873374939, acc: 0.6875 \n",
      "iteration: 64, loss: 0.8719428777694702, acc: 0.6171875 \n",
      "iteration: 72, loss: 0.8713812232017517, acc: 0.6328125 \n",
      "iteration: 80, loss: 0.8809607028961182, acc: 0.5859375 \n",
      "iteration: 88, loss: 0.8779277205467224, acc: 0.578125 \n",
      "iteration: 96, loss: 0.8822694420814514, acc: 0.6796875 \n",
      "iteration: 104, loss: 0.8872148990631104, acc: 0.5703125 \n",
      "iteration: 112, loss: 0.8899163603782654, acc: 0.609375 \n",
      "iteration: 120, loss: 0.8882603645324707, acc: 0.6328125 \n",
      "TRAINING: epoch: 35, loss: 0.8859574794769287, acc: 0.6201171875\n",
      "VALIDATION: epoch: 35, loss: 1.371503135189414, acc: 0.578125\n",
      "iteration: 0, loss: 0.8789991140365601, acc: 0.6796875 \n",
      "iteration: 8, loss: 0.8649685382843018, acc: 0.6875 \n",
      "iteration: 16, loss: 0.8571000695228577, acc: 0.578125 \n",
      "iteration: 24, loss: 0.8613385558128357, acc: 0.609375 \n",
      "iteration: 32, loss: 0.8571329116821289, acc: 0.6796875 \n",
      "iteration: 40, loss: 0.8683692812919617, acc: 0.5859375 \n",
      "iteration: 48, loss: 0.8712825775146484, acc: 0.578125 \n",
      "iteration: 56, loss: 0.8630971312522888, acc: 0.6640625 \n",
      "iteration: 64, loss: 0.8682305216789246, acc: 0.5234375 \n",
      "iteration: 72, loss: 0.871745228767395, acc: 0.6484375 \n",
      "iteration: 80, loss: 0.8744959235191345, acc: 0.6875 \n",
      "iteration: 88, loss: 0.8796762228012085, acc: 0.6015625 \n",
      "iteration: 96, loss: 0.8769277930259705, acc: 0.5859375 \n",
      "iteration: 104, loss: 0.8753489255905151, acc: 0.6484375 \n",
      "iteration: 112, loss: 0.8785525560379028, acc: 0.5078125 \n",
      "iteration: 120, loss: 0.877799391746521, acc: 0.6328125 \n",
      "TRAINING: epoch: 36, loss: 0.8794825673103333, acc: 0.61865234375\n",
      "VALIDATION: epoch: 36, loss: 1.332438038662076, acc: 0.578369140625\n",
      "iteration: 0, loss: 0.6712217926979065, acc: 0.6875 \n",
      "iteration: 8, loss: 0.8391656279563904, acc: 0.625 \n",
      "iteration: 16, loss: 0.8886922597885132, acc: 0.6484375 \n",
      "iteration: 24, loss: 0.8694437146186829, acc: 0.6328125 \n",
      "iteration: 32, loss: 0.8820340633392334, acc: 0.65625 \n",
      "iteration: 40, loss: 0.8881351947784424, acc: 0.6015625 \n",
      "iteration: 48, loss: 0.8985446095466614, acc: 0.5625 \n",
      "iteration: 56, loss: 0.8978526592254639, acc: 0.578125 \n",
      "iteration: 64, loss: 0.8918587565422058, acc: 0.6171875 \n",
      "iteration: 72, loss: 0.8887007236480713, acc: 0.671875 \n",
      "iteration: 80, loss: 0.8879370093345642, acc: 0.6171875 \n",
      "iteration: 88, loss: 0.8863725066184998, acc: 0.609375 \n",
      "iteration: 96, loss: 0.8825563788414001, acc: 0.71875 \n",
      "iteration: 104, loss: 0.8818202614784241, acc: 0.6171875 \n",
      "iteration: 112, loss: 0.881202220916748, acc: 0.7734375 \n",
      "iteration: 120, loss: 0.8811426758766174, acc: 0.640625 \n",
      "TRAINING: epoch: 37, loss: 0.8779561519622803, acc: 0.64111328125\n",
      "VALIDATION: epoch: 37, loss: 1.349340908229351, acc: 0.576904296875\n",
      "iteration: 0, loss: 0.8265820145606995, acc: 0.671875 \n",
      "iteration: 8, loss: 0.8513320684432983, acc: 0.6171875 \n",
      "iteration: 16, loss: 0.875232458114624, acc: 0.6796875 \n",
      "iteration: 24, loss: 0.8658069968223572, acc: 0.578125 \n",
      "iteration: 32, loss: 0.8682248592376709, acc: 0.6796875 \n",
      "iteration: 40, loss: 0.8681032061576843, acc: 0.6484375 \n",
      "iteration: 48, loss: 0.8678999543190002, acc: 0.6015625 \n",
      "iteration: 56, loss: 0.8741322159767151, acc: 0.609375 \n",
      "iteration: 64, loss: 0.8754415512084961, acc: 0.6796875 \n",
      "iteration: 72, loss: 0.879800021648407, acc: 0.6015625 \n",
      "iteration: 80, loss: 0.8728628158569336, acc: 0.6796875 \n",
      "iteration: 88, loss: 0.8740382194519043, acc: 0.65625 \n",
      "iteration: 96, loss: 0.8797595500946045, acc: 0.5546875 \n",
      "iteration: 104, loss: 0.8745395541191101, acc: 0.671875 \n",
      "iteration: 112, loss: 0.8758376240730286, acc: 0.5703125 \n",
      "iteration: 120, loss: 0.8757622838020325, acc: 0.6328125 \n",
      "TRAINING: epoch: 38, loss: 0.8778658509254456, acc: 0.63330078125\n",
      "VALIDATION: epoch: 38, loss: 1.3770490791648626, acc: 0.57470703125\n",
      "iteration: 0, loss: 0.74516361951828, acc: 0.6640625 \n",
      "iteration: 8, loss: 0.9113953113555908, acc: 0.59375 \n",
      "iteration: 16, loss: 0.8925169706344604, acc: 0.6953125 \n",
      "iteration: 24, loss: 0.8841264247894287, acc: 0.59375 \n",
      "iteration: 32, loss: 0.8916006684303284, acc: 0.609375 \n",
      "iteration: 40, loss: 0.8886308073997498, acc: 0.6171875 \n",
      "iteration: 48, loss: 0.8872267007827759, acc: 0.6171875 \n",
      "iteration: 56, loss: 0.8882111310958862, acc: 0.65625 \n",
      "iteration: 64, loss: 0.8822371363639832, acc: 0.6640625 \n",
      "iteration: 72, loss: 0.8791673183441162, acc: 0.625 \n",
      "iteration: 80, loss: 0.87605220079422, acc: 0.5859375 \n",
      "iteration: 88, loss: 0.8792967200279236, acc: 0.65625 \n",
      "iteration: 96, loss: 0.8820784687995911, acc: 0.640625 \n",
      "iteration: 104, loss: 0.8806828856468201, acc: 0.609375 \n",
      "iteration: 112, loss: 0.8784308433532715, acc: 0.6171875 \n",
      "iteration: 120, loss: 0.8753969669342041, acc: 0.6015625 \n",
      "TRAINING: epoch: 39, loss: 0.87814861536026, acc: 0.6279296875\n",
      "VALIDATION: epoch: 39, loss: 1.3121501505374908, acc: 0.583740234375\n",
      "Early stop at epoch 39!\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "early_stop = False\n",
    "epochs_no_improve = 0\n",
    "n_epochs_stop = 5\n",
    "min_loss = np.inf\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for n_epoch in range(N_EPOCHS):\n",
    "    loss_history = []\n",
    "    acc_history = []\n",
    "    val_loss = 0\n",
    "\n",
    "    # Training step\n",
    "    resnet.train()\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optim.zero_grad()\n",
    "        X, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        outputs = resnet(X)\n",
    "        y = y.long()\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        loss_history.append(loss.detach())\n",
    "\n",
    "        # Give some real-time indication for the user how the model is doing\n",
    "        if i % (BATCH_SIZE // 15)  == 0:\n",
    "            inds = torch.argmax(outputs, axis=1)\n",
    "            acc = (inds == y).sum() / len(y)\n",
    "            acc_history.append(acc)\n",
    "            print(f\"iteration: {i}, loss: {sum(loss_history) / len(loss_history)}, acc: {acc} \")\n",
    "    \n",
    "    mu_loss_train = sum(loss_history) / len(loss_history)\n",
    "    batch_acc_train = sum(acc_history) / len(acc_history)\n",
    "    print(f\"TRAINING: epoch: {n_epoch}, loss: {mu_loss_train}, acc: {batch_acc_train}\")\n",
    "      \n",
    "    # Validation step\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "    predictions = []\n",
    "    errors = []\n",
    "    resnet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            X, y = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "            outputs = resnet(X)\n",
    "            y = y.long()\n",
    "            loss = criterion(outputs, y)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            inds = torch.argmax(outputs, axis=1)\n",
    "            acc = (inds == y).sum() / len(y)\n",
    "            # Calculate where error happens, for tensorboard\n",
    "            error_inds = np.where((inds != y).cpu())\n",
    "            error_labels = y[error_inds[0]].cpu()\n",
    "\n",
    "            errors.append(error_labels)\n",
    "            predictions.append(inds)\n",
    "            val_acc_history.append(acc)\n",
    "            val_loss_history.append(loss.item())\n",
    "        \n",
    "        \n",
    "        # Update metrics\n",
    "        mu_loss_val = sum(val_loss_history) / len(val_loss_history)\n",
    "        batch_acc_val = sum(val_acc_history) / len(val_acc_history)\n",
    "        # Update scheduler\n",
    "        scheduler.step(mu_loss_val)\n",
    "        # Stack our List[Tensor()]\n",
    "        errors = torch.stack([e for arr in errors for e in arr])\n",
    "        predictions = torch.stack([p for arr in predictions for p in arr])\n",
    "        \n",
    "        # Write metrics to logs\n",
    "        writer.add_scalars(\"Loss\", {'train': mu_loss_train,\n",
    "                                    'val': mu_loss_val}, n_epoch)\n",
    "        writer.add_scalars(\"Accuracy\", {'train': batch_acc_train,\n",
    "                                        'val': batch_acc_val}, n_epoch)\n",
    "        writer.add_histogram(\"Validation prediction error distribution\", errors, n_epoch)\n",
    "        writer.add_histogram(\"Validation prediction distribution\", predictions, n_epoch)\n",
    "        writer.flush()\n",
    "\n",
    "        print(f\"VALIDATION: epoch: {n_epoch}, loss: {mu_loss_val}, acc: {batch_acc_val}\")\n",
    "        \n",
    "        # Early stop if loss doesn't improve\n",
    "        if mu_loss_train < min_loss:\n",
    "            # Save model\n",
    "            torch.save({\n",
    "            'epoch': n_epoch,\n",
    "            'model_state_dict': resnet.state_dict(),\n",
    "            'optimizer_state_dict': optim.state_dict(),\n",
    "            'loss': mu_loss_train,\n",
    "            }, f'{LOG_PATH}/{MODEL_PATH}/resnet50-batch_{BATCH_SIZE}_train_loss_{mu_loss_train}_val_{mu_loss_val}.pt')\n",
    "            min_loss = mu_loss_train\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == n_epochs_stop:\n",
    "                print(f\"Early stop at epoch {n_epoch}!\")\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "hl9G4XiUHeKk"
   ],
   "name": "ResNet-50.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
